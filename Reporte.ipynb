{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC para generación de texto coherente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Índice:\n",
    "* [Introducción](#intro)\n",
    "* [Metodología](#meth)\n",
    "* [Implementación y resultados](#imp-res)\n",
    "    * [Limpieza de Texto](#limp)\n",
    "    * [Matriz de Transición](#pmatriz)\n",
    "    * [Implementación del algoritmo Metropolis-Hasting](#mh)\n",
    "* [Propuesta: Sentiment Analysis](#sa)\n",
    "* [Conclusiones](#conc)\n",
    "* [Bibliografía](#ref)\n",
    "* [Anexos](#anex)\n",
    "    * [Código](#code)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción <a class=\"anchor\" id=\"intro\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; text-indent: 1em;\">Día con día, diversas aplicaciones nos ayudan a producir texto, ya sea Word ayudándonos a crear un documento sin errores gramaticales u ortográficos, Google dándonos resúmenes cortos y rápidos de páginas webs o hasta Siri que se comunica con nosotros de forma particularmente humana, pero ¿alguna vez te has preguntado cómo es que una computadora se encarga de \"dar vida\" a estas aplicaciones, que únicamente sabe pensar en ceros y unos, puede interpretar y producir lenguaje humano? El Procesamiento del Lenguaje Natural (NLP) es un campo de estudio de la inteligencia artificial que se enfoca en la interacción entre las computadoras y el lenguaje humano. Éste utiliza algoritmos y técnicas para comprender, interpretar y generar texto (Jurafsky & Martin, 2020). Una de las herramientas más utilizadas en el NLP son las cadenas de Márkov, los cuales son modelos matemáticos que representan la probabilidad de transición entre diferentes estados de un proceso estocástico.</div>\n",
    "\n",
    "<p style=\"text-align: justify;\">Si quisiéramos ir aún más lejos, podríamos implementar herramientas un poco más poderosas como el MCMC. El Muestreo de Márkov Chain Monte Carlo (MCMC) es una técnica de muestreo ampliamente utilizada en estadística y aprendizaje automático para estimar distribuciones de probabilidad y simular sistemas complejos (Robert & Casella, 2013). El objetivo de este proyecto es simular un texto que imite el estilo de escritura de un autor a elección del usuario. Pero esto puede volverse bastante complicado pues cada autor tiene su propia forma de escribir y expresarse, es decir, cada quién tiene sus vicios del lenguaje, tono, etc. De igual manera, no es lo mismo reproducir el texto plano de un libro que el de un tweet, ya que cada uno tiene sus dificultades de procesamiento. Mientras el texto de un libro puede llegar a ser complejo por las herramientas literarias que puede llegar a utilizar, un tweet igual puede mostrar dificultades por los recursos que utilizan como las menciones, hashtags, emojis, etc. La única ventaja es que los tweets son mensajes más cortos.</p>\n",
    "\n",
    "<p style=\"text-align: justify;\">En este reporte, describimos la implementación de un algoritmo de MCMC llamado Metropolis-Hastings para generar texto coherente a partir de un texto dado (o palabra). La generación de texto coherente es un problema relativamente difícil en el procesamiento del lenguaje natural y el aprendizaje automático, MCMC ofrece una forma sistemática y flexible de explorar diferentes combinaciones de palabras para generar texto significativo (Andrieu et al., 2003)</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodología <a class=\"anchor\" id=\"meth\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify; text-indent: 1em;\">El método MCMC (Markov Chain Monte Carlo) es una técnica de simulación numérica que se utiliza para estimar la distribución de probabilidad de una variable aleatoria (Robert & Casella, 2013). La idea detrás del método es construir una cadena de Markov que tenga como distribución estacionaria la distribución de probabilidad que se quiere estimar. </div>\n",
    "\n",
    "<p style=\"text-align: justify;\">Supongamos que nosotros queremos crear un texto aleatorio parecido de un autor X. Cada palabra que se genere dentro del texto seguirá una distribución teórica “π”, la cual deseamos simular para generar el texto. ¿Cómo podríamos construir una cadena ergódica que admita a π como distribución invariante? Dado un texto, podemos crear una matriz de transición o estados P que nos indique cuál es la probabilidad de pasar de una palabra dada a otra. Para ello proponemos el siguiente pseudocódigo:</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenizamos el texto en una lista\n",
    "\n",
    "\n",
    "2. Recorremos cada palabra de la lista\n",
    "\n",
    "\n",
    "3. Para cada palabra realizamos lo siguiente:\n",
    "    - Guardamos la siguiente palabra (ya sea en un array o diccionario) junto con un 1 que servirá de contador\n",
    "\n",
    "\n",
    "    - Si ya se había guardado esa palabra antes, le sumamos 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagrama 0](img/Diagram0.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al tener la matriz de transición y suponiendo que el texto es una cadena ergódica. Lo cual lo es, pues por definición una cadena ergódica es aquella en la cual existe alguna potencia de la matriz cuyas entradas sean todas estrictamente mayores que cero, y en nuestra cadena empezamos con contadores de 1. Entonces por el teorema que dice que la Cadena de Markov definida en Z con propuesta acorde a una caminata aleatoria Q y probabilidad de aceptación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\alpha_ij=min \\lbrace 1,\\pi_j/\\pi_i \\rbrace $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tenemos que, sean $i$ y $j$ palabras del texto:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi(j)=\\lim_{n \\to \\infty}p_{ij}^{(n)}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y por teorema ergódico, podemos estimar esperanzas de funciones utilizando promedios aritméticos y así simular la mejor opción para la siguiente palabra en el texto.\n",
    "\n",
    "Dado todo lo anterior, para generar un texto después de una palabra dada, primero podríamos proponer los candidatos que muestra la matriz $P$ y escoger alguno dada su distribución propuesta $\\pi_0$. Para que el texto converja a lo que queremos, es decir que acepte a $\\pi$ como distribución invariante, calculamos un coeficiente o probabilidad de La cuál lo es, pues por definición una cadena ergódica es aquella en la cual existe alguna potencia de la matriz cuyas entradas sean todas estrictamente mayores que cero, y en nuestra cadena empezamos con contadores de 1 aceptación para ver si nos quedamos con la palabra propuesta o no, lo cual nos creará una nueva probabilidad de transición. Esto se simulará para un número grande de iteraciones, ya que por TCL, entre más iteraciones se tengan, más preciso y de mejor calidad será el texto que se genere, pero obviamente tomará más tiempo. Para ello, proponemos el siguiente pseudocódigo para la generación de texto:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagrama 2](img/Diagram2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación y resultados <a class=\"anchor\" id=\"imp-res\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de Texto <a class=\"anchor\" id=\"limp\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\"> Antes de implementar el algoritmo hay varias cosas que se deben tomar en cuenta. Primero, debemos pensar si queremos filtrar el texto antes de aplicar el algoritmo, ya que los signos de puntuación, caracteres especiales (no ASCII) y hasta las letras mayúsculas pueden alterar la forma en la que se genera el texto. Un ejemplo de esto se puede ver en la imagen de abajo en como agregando un punto final puede tomar distintas connotaciones</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagrama 1](img/Diagram1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">Al escoger no filtrar nuestro texto, corremos el riesgo que nuestro texto no sea tan aleatorio como quisiéramos y que no sea del todo lógico o coherente. Por el otro lado, filtrarlo haría que la convergencia sea más rápida, creando un texto más conciso en el estilo deseado, pero sin ningún sentido lógico gramatical; por ejemplo, en español los acentos pueden hacer que todo el sentido de una palabra sea completamente distinto.Esta misma cuestión sobre el filtrado nos trajo una duda distinta: ¿qué sucedería si quisiéramos generar tweets? El problema de querer filtrar texto en un tweet es que al eliminar caracteres especiales como hashtags y menciones corremos el riesgo de romper completamente con la lógica del tweet. Y muchas veces estos se ocupan para generar que tu tweet tenga un mayor impacto y puedas generar una tendencia.</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Hashtags](img/tweet.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por eso mismo se implementó el algoritmo no únicamente con textos en español, sino también en inglés, aplicando en ambos casos un caso donde se ha filtrado el texto y uno en el que no. En el caso de los tweets, decidimos no filtrar nada para ver qué sucedía. \n",
    "\n",
    "Además, utilizamos textos literarios para ver la diferencia entre ellos y la generación de tweets .Los textos utilizados para la implementación fueron “Don Quijote de la Mancha” y “El retrato de Dorian Gray” ambos en formato de texto. Para los tweets se descargó una base de datos en formato csv de la página [TTA - Search](https://www.thetrumparchive.com/). Los códigos para la lectura de los archivos y la limpieza del texto se encuentran en el apartado de Código Adicional en Anexos y lo que hacen es leer los distintos archivos, ya sea limpiarlos o no, y crear una lista con las palabras los textos para que se puedan procesar luego."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de Transición <a class=\"anchor\" id=\"pmatriz\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">Para implementar MCMC en la generación de texto, primero construimos una matriz de transición que describe las probabilidades de transición entre palabras consecutivas en un texto. Para ello construimos una función que toma una lista de palabras y crea un diccionario anidado que representa la matriz de transición. Cada palabra en el texto se utiliza como una llave en el diccionario, y sus valores son otro diccionario que contiene las palabras que siguen a la palabra clave y la frecuencia relativa con la que aparecen. Esta matriz de transición se utiliza para guiar la generación de texto en el algoritmo de MCMC (ver Anexo).</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación del algoritmo Metropolis-Hasting <a class=\"anchor\" id=\"mh\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"text-align: justify;\">Para implementar el algoritmo, crearemos pimero una función alpha que calcule la probabilidad de aceptación de la nueva palabra y que penalice las transiciones poco probables, calculando el negativo del logaritmo de la probabilidad de transición, ya que son probabilidades, es decir p∈(0,1). Esta función, recibirá la palabra actual, la propuesta siguiente y la matriz de transición, devolviendo así una propabilidad de aceptación (ver Anexo).</p>\n",
    "\n",
    "<p style=\"text-align: justify;\">De esta forma primero muestrearemos una palabra de la distribución de probabilidad obtenida (la matriz o diccionario creado en la función anterior). Despues implementamos el algoritmo de Metropolis-Hastings para generar texto utilizando la matriz de transición P y la probabilidad de aceptación α. La función toma como entrada una palabra inicial (seed_word), la longitud deseada del texto generado y el número de iteraciones para realizar en cada paso del algoritmo (entre más iteraciones, más preciso) y regresa una string (variable que será nuestro texto). Al utilizar la probabilidad de aceptación y la alpha asociada con las palabras (ver Anexo), el algoritmo favorece las transiciones de palabras con mayor probabilidad, lo que puede resultar en un texto generado más coherente.</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Texto en español “Don Quijote”"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del texto de Don Quijote de la Mancha obtuvimos nuestra matriz de transición con la cual calculamos las probabilidades de pasar de una palabra dada a otra. Asimismo, generamos texto a partir de una palabra inicial “amarillo” y con un tamaño de “50” palabras. Nos salió el siguiente texto: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>Seed: \"amarillo\"; Tamaño: 50 </em><br><br>\n",
    "- <strong>Sin Filtrar</strong>: \"amarillo y sólo os conozco, por hecho con mucho cuando, movido a todos se llamaba Juan Rufo, jurado nada), que estáis tan buena fe se podrían reprensentallas, y los rostros son los desengaños no se contentaron de caballería y temerosas y la historia de rescate o a quien tiene usurpado; que\"\n",
    "\n",
    "El cual al ser español de España no nos genera mucho sentido. Por lo mismo, tuvimos que filtrar el texto quitando todas las comas, acentos, comillas y alteraciones que puedan modificar la connotación de nuestro texto. Esto lo quitamos porque en el español al poner una tilde en alguna palabra cambia radicalmente su connotación, nos queda el siguiente texto:\n",
    "\n",
    "- <strong>Texto Filtrado</strong>: \"amarillo y mas deseos y diciendo ah traidor que se le parecio y guy de aquella gran reina de llamar el primero de leer de caballerias que estaban la noticia del caballo rocinante albarde el de don quijote el cual ya pasados siglos y aunque la hacienda alli tendido en ningun\"\n",
    "\n",
    "Notamos que sigue el texto sin tener mucho sentido pues el español de España y del año 1605."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Texto en Inglés “Picture of Dorian Gray”"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del texto de Picture of Dorian Gray igualmente generamos un texto a partir de una palabra inicial “yellow” y con un tamaño de “50” palabras. Nos salió el siguiente texto:\n",
    "\n",
    "<em>Seed: \"yellow\"; Tamaño: 50 </em><br><br>\n",
    "\n",
    "- <strong>Sin Filtrar</strong>: yellow piazza of evil, with myriads of myself,\" said Dorian. Tell me know their coats and give him the aged, I to put her with dyed hair and you really changed? Or rather, I am afraid of, I am quite obvious. But it was, but that I can it began to\n",
    "\n",
    "Como podemos ver el texto tiene sentido únicamente en las 2 primeras oraciones después deja de tenerlos porque aparecen puntos y aparte. Lo que nos afecta porque ya son ideas que no tienen que ver con la palabra inicial. Por lo mismo, tenemos que implementar nuestro texto filtrado para que tenga una mayor coherencia lo generado por el código, nos queda el siguiente texto:\n",
    "\n",
    "- <strong>Texto Filtrado</strong>: yellow chinese box twentyseven i was nothing fearful about it all a burden to death of the women were pictured to see the emotion no you must come some strange conjectures as much better go through long _clarin_ of the room that it is like fire to linger sometimes think certainly\n",
    "\n",
    "Donde al igual que en texto de Don Quijote solo es coherente en las 2 primeras líneas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweet de Donald Trump"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>Seed: \"America\"; Tamaño:40 </em><br><br>\n",
    "\n",
    "- <strong>Texto</strong>: \"America could extort $1,000,000.00 from the crowd. He is willing to Create Jobs, Jobs, Border than a tax cuts &amp, @gatewaypundit. @jheil at R's. Shame! ....President. We need to establish a country could be winning their best opportunity to a great\"\n",
    "\n",
    "Aquí nos damos cuenta de que nuestro código tiene sentido cuando han sido obtenidos de pequeñas oraciones, ya que un tweet habla de lo mismo todo el tiempo. En este caso no se debería hacer un texto filtrado ya que perderíamos muchas palabras ocupadas en los textos y si quisiéramos ocuparlo para generar tendencia, no lo podríamos hacer. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propuesta: Sentiment Analysis <a class=\"anchor\" id=\"meth\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify;\">Como pudimos ver, una de las mejores formas en las que pudimos aplicar el algoritmo fue para generar Tweets, ya que le es fácil identificar patrones específicos en textos cortos (mediante la matriz de transición) y replicarlos, tareas que muchas veces hacen bots en Twitter para difundir mensajes de odio, sin ser detectados. Una forma de detección en esta plataforma es mediante “Sentiment Analysis” con la cual identifican si un tweet es de connotación positiva o negativa. Veamos si un bot de Donald Trump, creado con nuestro algoritmo pudiese ser detectado y fichado como discurso de odio.</p>\n",
    "\n",
    "<p style=\"text-align: justify;\">Primero, utilizaremos una paquetería de análisis de sentimiento ya entrenado de NLTK que, aunque no es tan preciso como otros modelos basados en transformadores como BERT de Google o GPT de OpenIA, es suficiente y no requiere de muchos recursos computacionales. Después, generaremos varios tweets de datos filtrados y no filtrados y los analizaremos. Cabe mencionar, que en ambos casos filtraremos los tweets antes de realizar el análisis de sentimiento, ya que la paquetería puede regresar valores extraños si trabaja con hashtags o arrobas.</p>\n",
    "\n",
    "Los resultados fueron los siguientes:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets generados con distintas semillas:\n",
    "\n",
    "- <strong>Seed: \"America\"; Tamaño:40:</strong> \n",
    "  - \"America great having a fully recover from Trump has come the stupidity of Crazy union backs of violent extremists. Really nice! #MakeAmericaGreatAgain Clear path of modern history by si… RT @realDonaldTrump: Elect @realDonaldTrump on my live from our bald and NOT\"\n",
    "  - \"America And to Covid patients. Said he wasted billions of votes than the President. Business owners and Total Endorsement! .@Denver4VA of jobs” http://t.co/tEmQD0xQY5 via @HuffPostGreen The CDC continues to help of Protecting America getting caught lying &amp, failure http://t.co/SV6KAS1E Isn’t it\"\n",
    "\n",
    "- <strong>Seed: \"China\"; Tamaño:40:</strong>\n",
    "  - \"China should always knew never change.\" \"\"\"@TheHawk4221: @realDonaldTrump Let's do not what you’re Vladimir Putin #Trump2016\" \"\"\"@Joey_Columbo: @realDonaldTrump for all. The two great job Heather! \"\"\"@IanAnderson13: If your idea flow of public course as President.\"\"\" \"\"\"@MeetinLongBeach: @TrumpGolfLA feat. @RepAdamSchiff today.Under Nadler’s\"\n",
    "  - \"China stolen by the Democrats didn’t get in Congress a confirmation is very lonely. (cont) http://t.co/pAhyD1tT I'll bet trump has become an emergency deliveries will be one .\"\"\" \"\"\"@05FXDLI: @realDonaldTrump greatest economy when 2012 Pageant in swing states to be sleazebags\"\n",
    "\n",
    "- <strong>Seed: \"democrats\"; Tamaño:40:</strong>\n",
    "  - \"democrats too.\" Weakness is such men!\" I even have been sent to the Fake News https://t.co/6pZQ… RT @wesbury: Texas suit. See you all. https://t.co/gsFSghkmdM RT @hughhewitt: Elite Nancy Pelosi, who have a great all credibility was created and ready. \".@Mediaite:\"\"Donald Trump\"\n",
    "  - \"democrats are many a presidential failure. He isn't a lot of the best book \"\"Undisputed Truth\"\" &amp, frisk works. Scary!\" \"\"\"@LaurenDa123: @realDonaldTrump I wanted to pay!\"\" She visited Trump buys our country and @FLOTUS Melania and the most beautiful bargain. Congratulations\"\n",
    "\n",
    "- <strong>Seed: \"Mexico\"; Tamaño:40:</strong>\n",
    "  - \"Mexico in office. The young people coming as we continue this tragic terrorist on the United States, His Obama Briefed About the Border, Military, ISIS, illegal and shouldn’t vote for 1 woman will never seen before. Jobs, Jobs. He makes a\"\n",
    "  - \"Mexico is any failure up to make me telling if you're funnier and other requested by Ground Zero Credibility! @IngrahamAngle interview last day tweeting my total fraud they must repeal ObamaCare. Money From Start Naming Names’ http://t.co/7uzPNPJ9vK\" Via @espn: Donald Trump\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizando el análisis de sentimiento, despues de filtrar los textos, obtenemos los siguientes resultados en el orden de arriba:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Seed: \"America\"</strong>\n",
    "-   positivo\n",
    "-   negativo\n",
    "\n",
    "<strong>Seed: \"China\"</strong>\n",
    "- positivo\n",
    "- positivo\n",
    "\n",
    "<strong>Seed: \"democrats\"</strong>\n",
    "- positivo\n",
    "- positivo\n",
    "\n",
    "<strong>Seed: \"Mexico\"</strong>\n",
    "- negativo\n",
    "- negativo\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones <a class=\"anchor\" id=\"conc\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo Metropolis-Hastings es bastante bueno generar muestras de distribuciones de probabilidad no triviales desconocidas (como los textos en nuestro caso) mediante la construcción de una cadena de Markov cuya distribución estacionaria coincide con la distribución objetivo. Como pudimos ver, es relativamente fácil de implementar y converge relativamente rápido si se tiene una muestra grande de datos, regalando resultados no tan malos. De igual manera se puede adaptar a una amplia variedad de problemas.\n",
    "\n",
    "En nuestro caso se podría ocupar para la generación de tweets de cualquier persona que anteriormente haya tenido una gran cantidad de publicaciones hechas. Asimismo, podríamos simular el próximo tweet de la persona y poder contratacar con un tweet hecho por nosotros viendo cuales temas se les dificulta escribir o no escribe nada. También tiene implementaciones como para hacer boots en respuesta algún suceso trading, sin ser necesariamente captados por análisis de sentimiento de la plataforma.\n",
    "Sin embargo, el algoritmo Metropolis-Hastings también tiene algunas desventajas. En primer lugar, puede ser ineficiente en ciertos casos, ya que la convergencia a la distribución estacionaria puede ser lenta. La elección de una distribución propuesta adecuada es crucial para la eficiencia del algoritmo, pero esto puede ser difícil en la práctica. \n",
    "Algunas formas en las que se mejoraría el MCMC para la generación de texto pueden ser las siguientes: \n",
    "\n",
    "1. Utilizar un modelo de lenguaje más avanzado, como un modelo de n-gramas o un modelo neuronal, para obtener mejores estimaciones de las probabilidades de transición entre palabras. \n",
    "2. Mejorar la generación de texto utilizando modelos más avanzados, como redes neuronales recurrentes (RNN).\n",
    "3. Boots y elecciones.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencias <a class=\"anchor\" id=\"ref\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Repositorio del proyecto\n",
    "* https://github.com/Buebito/MCMC-Text-Generation.git\n",
    "##### Bibliografía\n",
    "* Andrieu, C., de Freitas, N., Doucet, A. et al. An Introduction to MCMC for Machine Learning. Machine Learning 50, 5–43 (2003). https://doi.org/10.1023/A:1020281327116\n",
    "* Gerlach, M. and Font-Clos, F. (2020) “A standardized project gutenberg corpus for statistical analysis of Natural Language and Quantitative Linguistics,” Entropy, 22(1), p. 126. Available at: https://doi.org/10.3390/e22010126. \n",
    "* Jurafsky, D., & Martin, J. H. (2020). Speech and Language Processing. An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition (3rd edition).\n",
    "* Robert, C.; Casella, G. (2010). Introducing Monte Carlo Methods with R. Berlin: Springer-Verlag.\n",
    "* (2018). Text generation using Monte-Carlo Sampling.\n",
    "\n",
    "##### Archivos externos\n",
    "* Brendan (2016) “The Trump Archive.” Available at: https://www.thetrumparchive.com. \n",
    "* Jesús  Darío (2017) El quijote en Texto Plano, El Quijote. GitHub. Available at: https://gist.github.com/jsdario/6d6c69398cb0c73111e49f1218960f79 (Accessed: April 21, 2023). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anexos <a class=\"anchor\" id=\"anex\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Jupyter Notebook completo del proyecto con todo el código y los resultados obtenidos, así como este reporte, los archivos txt y la base de datos csv se pueden encontrar en el repositorio de github [MCMC-Text-Generation](https://github.com/Buebito/MCMC-Text-Generation.git), el cual se seguirá actualizando."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código <a class=\"anchor\" id=\"code\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paqueterías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "#Paqueterías de filtrado\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "from collections.abc import MutableMapping\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lectura de texto y Tokenizado en lista de palabras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sin limpieza de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abre el archivo txt y lo convierte en una lista de palabras\n",
    "with open(\"El Quijote.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    content = file.read()\n",
    "    words = content.split()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Con limpieza de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recibe una string y devuelve la string sin puntuaciones o signos raros.\n",
    "\n",
    "def limpieza(text):\n",
    "#Unidecode toma un objeto de cadena, que posiblemente contenga caracteres no ASCII, y devuelve una cadena que se puede \n",
    "#codificar de forma segura en ASCII. En este caso se utilizó para remover acentos y emojis\n",
    "    text = unidecode(text)\n",
    "#Minúsculas\n",
    "    text = text.lower()\n",
    "#Eliminar signos de interrogación, exclamación y otros\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "#----------------------\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abre el texto y se guarda en string\n",
    "file = open(\"El Quijote.txt\", \"r\", encoding=\"utf-8\")\n",
    "text = file.read()\n",
    "file.close()\n",
    "#Lo limpia y lo tokeniza\n",
    "text=limpieza(text)\n",
    "words = text.split()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lectura de CVS (como dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets_01-08-2021.csv')\n",
    "\n",
    "#función que transforma una columna de un dataframe en un texto plano\n",
    "def colToText(df_columna):\n",
    "    l=list(df_columna)\n",
    "    return \" \".join(l)\n",
    "\n",
    "#lo convertimos en lista de palabras\n",
    "text=colToText(df.text)\n",
    "words=text.split()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matriz de Transición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que recibe un texto en forma de lista y te calcula la frecuencia con la que aparecen ciertas palabras\n",
    "# despues de una palabra específica\n",
    "def probabilidadesTransición(words):\n",
    "# P será nuestro diccionario que tendrá como llave todas las palabras disponibles en el texto\n",
    "# y como valor OTRO DICCIONARIO, cuyo llave volverán a ser todas las palabras disponibles en el texto\n",
    "# y como valor la probabilidad que despues de la primera palabra (primera llave) siga la segunda palabra\n",
    "# (segunda llave)\n",
    "    P = {}\n",
    "#Recorremos cada palabra en la lista y analizamos la primera palabra y su sucesora\n",
    "    for i in range(len(words)-1):\n",
    "#X: presente, Y: futuro\n",
    "        X = words[i]\n",
    "        Y = words[i+1]\n",
    "#Si x no está en el diccionario, la agregamos y agregamos como valor otro diccionario incluyendo como llave a Y y agregarle\n",
    "# 1 a la frecuencia en la que aparece Y despues de X\n",
    "        if P.get(X) is None:\n",
    "            P[X] = {}\n",
    "            P[X][Y] = 1\n",
    "#Si X ya está, ahora checamos si Y está como llave en el diccionario en el valor de X\n",
    "        else:\n",
    "            if P[X].get(Y) is None:\n",
    "#Si no está ponemos como valor 1 y si sí está le agregamos 1 a la frecuencia\n",
    "                P[X][Y] = 1\n",
    "            else:\n",
    "                P[X][Y] += 1\n",
    "#Teniendo ya el diccionario con las frecuencias, ahora toca sacar las probabilidades de que salga cada palabra\n",
    "#Recorremos cada llave del diccionario y sumamos todos los valores encontrados en el diccionario de esa llave\n",
    "    for i in P.keys():\n",
    "        s = float(sum(P[i].values()))\n",
    "#Ahora trecorremos cada llave del segundo diccionario (del valor de la primera llave) y ajustamos el valor de la frecuencia\n",
    "#dividiendola entre la suma anterior y asi obteniendo una probabilidad elemento del (0,1)\n",
    "        for k in P[i].keys():\n",
    "            P[i][k] = P[i][k]/s\n",
    "#Devolvemos la matriz de transición\n",
    "    return P"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha(P, current_word, next_word):\n",
    "\n",
    "#Este filtro es importante en este caso que no filtramos el texto (En el caso de no filtrar el texto no es neceario el if), ya que sucedía\n",
    "#muchas veces que la función estaba tratando de acceder a una llave existe en el diccionario o palabra en la matriz de probabilidades. \n",
    "#Esto sucedía por palabras que incluían un come o algun signo de admiración o exclamación.\n",
    "\n",
    "#Para corregir este error, se agregó una verificación en la función para asegurarnos de que solo intente acceder a claves existentes \n",
    "#en el diccionario P. Si la clave no existe, se devuelve un valor muy alto para que esa transición sea poco probable.\n",
    "    if current_word not in P or next_word not in P[current_word]:\n",
    "        return float('inf')\n",
    "    return -math.log(P[current_word][next_word])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_mh(P, seed_word, length, iteraciones):\n",
    "\n",
    "#Creamos una lista donde almacenaremos las palabras que contendrá el texto.\n",
    "#La variable current_word cambiará conforme vayamos agregando palabras al texto, empezando por la semilla\n",
    "    current_word = seed_word\n",
    "    text = [current_word]\n",
    "\n",
    "#Recorremos el largo que queremos que sea el texto\n",
    "    for i in range(length):\n",
    "\n",
    "#Vemos si la palabra se encuentra en el diccionario, es decir, si el autor utilizaría esa palabra en alguno de sus textos\n",
    "#Si no es el caso, tronamos la función\n",
    "        if P.get(current_word) is None:\n",
    "            print(\"Saavedra jamás diría eso\")\n",
    "            break\n",
    "\n",
    "#Si la palabra sí está en el diccionario (matriz), comenzamos el algoritmo Metropolis-Hasting para muestrear la siguiente palabra en la cadena\n",
    "#Esto lo haremos el numero de iteraciones que se desee\n",
    "        for i in range(iteraciones):\n",
    "\n",
    "#Dada la palabra en la que nos encontremos, sacaremos del diccionario las palabras que le pueden seguir como una lista, y la\n",
    "#probabilidad como otra\n",
    "#Recordemos que la matriz de transiciones es un diccionario de diccionarios, entonces al buscar la palabra actual current_word\n",
    "#en el diccionario, nos mostrará otro diccionario con las palabras que le pueden seguir como llaves y como valores las probabilidades\n",
    "            next_word_candidates = list(P[current_word].keys())\n",
    "            next_word_probabilities = list(P[current_word].values())\n",
    "\n",
    "#Teniendo la lista de palabras candidatas y probabilidades, seleccionamos una palabra utilizando la distribución propuesta \n",
    "#En este caso, proponemos distribución del modelo que sacamos anterirmente, es decir, utilizando la matriz de trancisiones (diccionario)\n",
    "#También podríamos proponer que la distribución fuera uniforme para todas las palabras, pero tendría un tiempo de convergencia menor.\n",
    "            proposed_next_word = random.choices(next_word_candidates, weights=next_word_probabilities, k=1)[0]\n",
    "\n",
    "#Calculamos la razón de aceptación con la función alpha\n",
    "# \"Lanzamos la moneda\" y vemos si nos quedamos con la palabra o nos movemos a otra\n",
    "\n",
    "# Probabilidad de no cambiar\n",
    "            a = alpha(P, current_word, text[-1])\n",
    "# Probabilidad de cambio a siguiente palabra\n",
    "            a2 = alpha(P, current_word, proposed_next_word)\n",
    "#Calculamos la propabilidad de aceptación\n",
    "            probabilidad_aceptacion = min(1, math.exp(a - a2))\n",
    "\n",
    "# Vemos si aceptamos o rechazamos la palabra propuesta, generando valores de la distribucion Unif(0,1) y viendo si es menor a \n",
    "# Mi probabilidad de aceptación\n",
    "            if random.random() < probabilidad_aceptacion:\n",
    "#Si es mayor, definimos la nueva palabra\n",
    "                next_word = proposed_next_word\n",
    "                break\n",
    "#Al terminar las iteraciones agregamos la nueva palabra a la lista\n",
    "        text.append(next_word)\n",
    "#Cambiamos la variable current_word a la nueva palabra agregada para seguir con el ciclo otra vez\n",
    "        current_word = next_word\n",
    "\n",
    "#Terminando el algoritmo, regresamos el texto todo junto\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis de Sentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recibe una lista con los tweets y regresa si es positivo, negativo o neutro\n",
    "def sa_tweet(tweets):\n",
    "    for tweet in tweets:\n",
    "        analysis = TextBlob(tweet)\n",
    "        if analysis.sentiment.polarity > 0:\n",
    "            print(f\"'{tweet}' es positivo\")\n",
    "        elif analysis.sentiment.polarity < 0:\n",
    "            print(f\"'{tweet}' es negativo\")\n",
    "        else:\n",
    "            print(f\"'{tweet}' es neutro\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ejemplo de cómo implementar las funciones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo ya la lista de palabras, sea filtrada o no, creamos el modelo de la cadena de Márkov, es decir la matriz de transición, con la función <em>probabilidadesTransición()</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = probabilidadesTransición(words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teniendo el modelo o la matriz P ya lista, creamos una variable <em>seed_word</em> que será la primera palabra con la que comenzará el texto. También sería prudente considerar crear una variable <em>length</em>, para el tamaño del texto deseado (tamaño en palabras) e <em>iteraciones</em>, para el numero de iteraciones que se deseen para el Metropolis-Hasting.\n",
    "\n",
    "Una vez teniendo las variables, las agregamos a la función <em>generate_text_mh<em> para que nos genere el texto deseado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saavedra jamás diría eso\n",
      "el cero no es natural\n"
     ]
    }
   ],
   "source": [
    "seed_word = \"el cero no es natural\"\n",
    "generated_text = generate_text_mh(modelo, seed_word, length=50, iteraciones=100000)\n",
    "print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ejemplo de implementación de sentiment análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'america great having a fully recover from trump has come the stupidity of crazy union backs of violent extremists really nice makeamericagreatagain clear path of modern history by si rt realdonaldtrump elect realdonaldtrump on my live from our bald and not' es positivo\n",
      "'america and to covid patients said he wasted billions of votes than the president business owners and total endorsement denver4va of jobs httptcotemqd0xqy5 via huffpostgreen the cdc continues to help of protecting america getting caught lying amp failure httptcosv6kas1e isnt it' es negativo\n",
      "'china should always knew never changethehawk4221 realdonaldtrump lets do not what youre vladimir putin trump2016joey_columbo realdonaldtrump for all the two great job heatheriananderson13 if your idea flow of public course as presidentmeetinlongbeach trumpgolfla feat repadamschiff todayunder nadlers' es positivo\n",
      "'china stolen by the democrats didnt get in congress a confirmation is very lonely cont httptcopahyd1tt ill bet trump has become an emergency deliveries will be one 05fxdli realdonaldtrump greatest economy when 2012 pageant in swing states to be sleazebags' es positivo\n",
      "'democrats too weakness is such men i even have been sent to the fake news httpstco6pzq rt wesbury texas suit see you all httpstcogsfsghkmdm rt hughhewitt elite nancy pelosi who have a great all credibility was created and ready mediaitedonald trump' es positivo\n",
      "'democrats are many a presidential failure he isnt a lot of the best book undisputed truth amp frisk works scarylaurenda123 realdonaldtrump i wanted to pay she visited trump buys our country and flotus melania and the most beautiful bargain congratulations' es positivo\n",
      "'mexico is any failure up to make me telling if youre funnier and other requested by ground zero credibility ingrahamangle interview last day tweeting my total fraud they must repeal obamacare money from start naming names' es negativo\n",
      "'mexico in office the young people coming as we continue this tragic terrorist on the united states his obama briefed about the border military isis illegal and shouldnt vote for 1 woman will never seen before jobs jobs he makes a' es negativo\n"
     ]
    }
   ],
   "source": [
    "lista=[\"America great having a fully recover from Trump has come the stupidity of Crazy union backs of violent extremists. Really nice! #MakeAmericaGreatAgain Clear path of modern history by si… RT @realDonaldTrump: Elect @realDonaldTrump on my live from our bald and NOT\",\"America And to Covid patients. Said he wasted billions of votes than the President. Business owners and Total Endorsement! .@Denver4VA of jobs http://t.co/tEmQD0xQY5 via @HuffPostGreen The CDC continues to help of Protecting America getting caught lying &amp, failure http://t.co/SV6KAS1E Isn’t it\",\"China should always knew never change.@TheHawk4221: @realDonaldTrump Let's do not what you’re Vladimir Putin #Trump2016@Joey_Columbo: @realDonaldTrump for all. The two great job Heather!@IanAnderson13: If your idea flow of public course as President.@MeetinLongBeach: @TrumpGolfLA feat. @RepAdamSchiff today.Under Nadler’s\",\"China stolen by the Democrats didn’t get in Congress a confirmation is very lonely. (cont) http://t.co/pAhyD1tT I'll bet trump has become an emergency deliveries will be one .@05FXDLI: @realDonaldTrump greatest economy when 2012 Pageant in swing states to be sleazebags\",\"democrats too. Weakness is such men! I even have been sent to the Fake News https://t.co/6pZQ… RT @wesbury: Texas suit. See you all. https://t.co/gsFSghkmdM RT @hughhewitt: Elite Nancy Pelosi, who have a great all credibility was created and ready. .@Mediaite:Donald Trump\",\"democrats are many a presidential failure. He isn't a lot of the best book Undisputed Truth &amp, frisk works. Scary!@LaurenDa123: @realDonaldTrump I wanted to pay!\"\" She visited Trump buys our country and @FLOTUS Melania and the most beautiful bargain. Congratulations\",\"Mexico is any failure up to make me telling if you're funnier and other requested by Ground Zero Credibility! @IngrahamAngle interview last day tweeting my total fraud they must repeal ObamaCare. Money From Start Naming Names’\",\"Mexico in office. The young people coming as we continue this tragic terrorist on the United States, His Obama Briefed About the Border, Military, ISIS, illegal and shouldn’t vote for 1 woman will never seen before. Jobs, Jobs. He makes a\"]\n",
    "l=[]\n",
    "for i in lista:\n",
    "    l.append(limpieza(i))\n",
    "sa_tweet(l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
